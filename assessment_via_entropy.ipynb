{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc4ec5-2532-4f35-ace9-af90dc3d30d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import attacks\n",
    "from datasets import ADULT, German, HealthHeritage, Lawschool\n",
    "from attacks.ensembling import pooled_ensemble\n",
    "from attacks.inversion_losses import _cosine_similarity_loss\n",
    "from attacks import invert_grad\n",
    "import torch\n",
    "from models import FullyConnected\n",
    "from utils import calculate_entropy_heat_map, Timer, match_reconstruction_ground_truth, categorical_accuracy_continuous_tolerance_score, categorical_softmax, post_process_continuous\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import kendalltau\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f6f24-00cc-46af-9369-b067ed76be2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results on Assessment via Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd8ae5-ffe6-4745-adb5-0c60b46b5e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the dataset and rerun the cells below to display the results corresponding to the chosen dataset\n",
    "# chose from 'ADULT', 'German', 'Lawschool', 'HealthHeritage'\n",
    "dataset_name = 'ADULT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70f636-5556-4ba2-93a1-0c3e0afc8eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the setting\n",
    "n_samples = 50\n",
    "size_of_ensemble = 30\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "# load the network\n",
    "with open(f'experiment_data/large_scale_experiments/{dataset_name}/experiment_46/metadata_46_{dataset_name}_{n_samples}_{size_of_ensemble}_1500_128_0.319_42_epoch0/net.pickle', 'rb') as f:\n",
    "    net = pickle.load(f)\n",
    "\n",
    "# load all the data\n",
    "reconstruction_ensembles_over_batch_size = []\n",
    "ground_truths_over_batch_size = []\n",
    "ground_truth_labels_over_batch_size = []\n",
    "orig_recons_over_batch_size = []\n",
    "for batch_size in batch_sizes:\n",
    "    path = f'experiment_data/large_scale_experiments/{dataset_name}/experiment_46/metadata_46_{dataset_name}_{n_samples}_{size_of_ensemble}_1500_128_0.319_42_epoch0/batch_size_{batch_size}/'\n",
    "    reconstruction_ensembles = [[torch.tensor(np.load(path + f'all_reconstructions_{sample}/ensemble_recon_{num}.npy')) for num in range(size_of_ensemble)] for sample in range(n_samples)]\n",
    "    ground_truths = [torch.tensor(np.load(path + f'ground_truth_{batch_size}_{sample}.npy')) for sample in range(n_samples)]\n",
    "    orig_recons = [torch.tensor(np.load(path + f'reconstruction_{batch_size}_{sample}.npy')) for sample in range(n_samples)]\n",
    "    ground_truth_labels = [torch.tensor(np.load(path + f'true_labels_{batch_size}_{sample}.npy')).long() for sample in range(n_samples)]\n",
    "    reconstruction_ensembles_over_batch_size.append(reconstruction_ensembles)\n",
    "    ground_truths_over_batch_size.append(ground_truths)\n",
    "    ground_truth_labels_over_batch_size.append(ground_truth_labels)\n",
    "    orig_recons_over_batch_size.append(orig_recons)\n",
    "print('Data loaded')\n",
    "\n",
    "base_path = f'experiment_data/large_scale_experiments/{dataset_name}/experiment_46/inversion_data_all_46_{dataset_name}_50_30_1500_{batch_sizes[-1]}_0.319_42.npy'\n",
    "stuff = np.load(base_path)\n",
    "\n",
    "\n",
    "# initialize the dataset\n",
    "print('Instantiating the dataset')\n",
    "if dataset_name == 'ADULT':\n",
    "    dataset = ADULT()\n",
    "elif dataset_name == 'German':\n",
    "    dataset = German()\n",
    "elif dataset_name == 'Lawschool':\n",
    "    dataset = Lawschool()\n",
    "elif dataset_name == 'HealthHeritage':\n",
    "    dataset = HealthHeritage()\n",
    "else:\n",
    "    raise ValueError('No such dataset')\n",
    "print('Dataset instantiated')\n",
    "dataset.standardize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40a6f4-e5f9-4623-a838-a8e3ccdcb01d",
   "metadata": {},
   "source": [
    "The next cell calculates the error bitmaps and the entropy maps for each feature in each reconstruction (50 samples in each batch size). For this, first the 30 reconstructions in the assembles are matched to the minimum loss reconstruction. Then, the reconstruction are pooled. Finally, we calculate the entropy and the error maps from the pooled reconstruction. The first run of this cell might take up to a few hours for each dataset, but any later runs will be faster, as the results are automatically saved and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88afbd-5937-42dc-aebe-e0580c49f9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we want to get the error maps and the entropy maps for each reconstruction\n",
    "error_maps_over_batch_size = []\n",
    "entropy_maps_over_batch_size = []\n",
    "decoded_ground_truths_over_batch_size = []\n",
    "decoded_reconstructions_over_batch_size = []\n",
    "\n",
    "timer = Timer(len(batch_sizes)*n_samples)\n",
    "\n",
    "if os.path.isfile(f'experiment_data/large_scale_experiments/{dataset_name}/experiment_46/metadata_46_{dataset_name}_{n_samples}_{size_of_ensemble}_1500_128_0.319_42_epoch0/error_maps_over_batch_size.pickle'):\n",
    "    with open(f'experiment_data/large_scale_experiments/{dataset_name}/experiment_46/metadata_46_{dataset_name}_{n_samples}_{size_of_ensemble}_1500_128_0.319_42_epoch0/error_maps_over_batch_size.pickle', 'rb') as f:\n",
    "        error_maps_over_batch_size = pickle.load(f)\n",
    "\n",
    "    with open(f'experiment_data/large_scale_experiments/{dataset_name}/experiment_46/metadata_46_{dataset_name}_{n_samples}_{size_of_ensemble}_1500_128_0.319_42_epoch0/entropy_maps_over_batch_size.pickle', 'rb') as f:\n",
    "        entropy_maps_over_batch_size = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    for i, batch_size in enumerate(batch_sizes):\n",
    "        error_maps = []\n",
    "        entropy_maps = []\n",
    "        decoded_ground_truths = []\n",
    "        decoded_reconstructions = []\n",
    "        errors = []\n",
    "    \n",
    "        for j in range(n_samples):\n",
    "            timer.start()\n",
    "            print(timer, end='\\r')\n",
    "            # --------- calculate minimum loss sample --------- #\n",
    "            losses = []\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            outputs = net(ground_truths_over_batch_size[i][j])\n",
    "            loss = criterion(outputs, ground_truth_labels_over_batch_size[i][j])\n",
    "            true_grad = [grad.detach() for grad in torch.autograd.grad(loss, net.parameters(), create_graph=True)]\n",
    "            for k in range(size_of_ensemble):\n",
    "                # calculate the guessed gradient\n",
    "                outputs = net(categorical_softmax(reconstruction_ensembles_over_batch_size[i][j][k], dataset))\n",
    "                loss = criterion(outputs, ground_truth_labels_over_batch_size[i][j])\n",
    "                guessed_gradient = [grad.detach() for grad in torch.autograd.grad(loss, net.parameters(), create_graph=True)]\n",
    "                loss = _cosine_similarity_loss(guessed_gradient, true_grad, device='cpu').item()\n",
    "                losses.append(loss)\n",
    "            min_index = np.argmin(np.array(losses)).item()\n",
    "            min_loss_sample = reconstruction_ensembles_over_batch_size[i][j][min_index]\n",
    "            # --------- minimum loss sample calculated --------- #\n",
    "        \n",
    "            # --------- reorder wrt minimum sample --------- #\n",
    "            reconstructions_decoded = [dataset.decode_batch(rec.detach().clone()) for rec in reconstruction_ensembles_over_batch_size[i][j]]\n",
    "            tolerance_map = dataset.create_tolerance_map()\n",
    "            all_indices_match = []\n",
    "            for reconstruction in reconstructions_decoded:\n",
    "                _, _, _, _, _, indices = match_reconstruction_ground_truth(dataset.decode_batch(min_loss_sample.detach().clone()),\n",
    "                                                                           reconstruction, tolerance_map=tolerance_map,\n",
    "                                                                           return_indices=True, match_based_on='all')\n",
    "                all_indices_match.append(indices)\n",
    "            reordered_reconstructions = torch.stack([rec[idx].detach().clone() for rec, idx in zip(reconstruction_ensembles_over_batch_size[i][j], all_indices_match)])\n",
    "        \n",
    "            resulting_reconstruction_for_entropy, cont_stds = pooled_ensemble(reconstructions=reordered_reconstructions, match_to_batch=min_loss_sample, dataset=dataset, pooling='soft_avg+softmax', return_std=True, already_reordered=True)\n",
    "            resulting_reconstruction, _ = pooled_ensemble(reconstructions=reordered_reconstructions, match_to_batch=min_loss_sample, dataset=dataset, pooling='median+softmax', return_std=True, already_reordered=True)\n",
    "            decoded_reconstruction, decoded_ground_truth = dataset.decode_batch(resulting_reconstruction, standardized=True), dataset.decode_batch(ground_truths_over_batch_size[i][j], standardized=True)\n",
    "            reordered_reconstruction, _, _, _, _, idx = match_reconstruction_ground_truth(decoded_ground_truth, decoded_reconstruction, tolerance_map=dataset.create_tolerance_map(), return_indices=True)\n",
    "            entropy_heat_map, _ = calculate_entropy_heat_map(resulting_reconstruction_for_entropy[idx], ground_truths_over_batch_size[i][j], cont_stds[idx], dataset)\n",
    "            _, error_heat_map = calculate_entropy_heat_map(post_process_continuous(resulting_reconstruction[idx], dataset), ground_truths_over_batch_size[i][j], cont_stds[idx], dataset)\n",
    "            decoded_ground_truths.append(decoded_ground_truth)\n",
    "            decoded_reconstructions.append(reordered_reconstruction)\n",
    "            error_maps.append(error_heat_map)\n",
    "            entropy_maps.append(entropy_heat_map)\n",
    "            errors.append(np.mean(error_heat_map))\n",
    "            timer.end()\n",
    "        error_maps_over_batch_size.append(error_maps)\n",
    "        entropy_maps_over_batch_size.append(entropy_maps)\n",
    "        decoded_ground_truths_over_batch_size.append(decoded_ground_truths)\n",
    "        decoded_reconstructions_over_batch_size.append(decoded_reconstructions)\n",
    "        print(f'{batch_size} {1-np.mean(errors):.4f} {np.std(errors):.4f}')\n",
    "        \n",
    "    with open(f'experiment_data/large_scale_experiments/{dataset_name}/experiment_46/metadata_46_{dataset_name}_{n_samples}_{size_of_ensemble}_1500_128_0.319_42_epoch0/error_maps_over_batch_size.pickle', 'wb') as f:\n",
    "        pickle.dump(error_maps_over_batch_size, f)\n",
    "\n",
    "    with open(f'experiment_data/large_scale_experiments/{dataset_name}/experiment_46/metadata_46_{dataset_name}_{n_samples}_{size_of_ensemble}_1500_128_0.319_42_epoch0/entropy_maps_over_batch_size.pickle', 'wb') as f:\n",
    "        pickle.dump(entropy_maps_over_batch_size, f)\n",
    "timer.duration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08025194-eca8-4124-8cff-f03fbfc9b74f",
   "metadata": {},
   "source": [
    "## Results of quantiling based on entropy, as table 4 in the main paper, and tables 29-32 in the Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41643f-1b0d-4fc3-b4c8-7b30111e0348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_indices = dataset.train_cat_indices\n",
    "cont_indices = dataset.train_cont_indices\n",
    "percentile = 0.25 # change to display different results -- in the paper we use 0.25\n",
    "display_data = np.zeros((len(batch_sizes), 5), dtype='object')\n",
    "\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "    cont_top_errors = []\n",
    "    cont_bottom_errors = []\n",
    "    cat_top_errors = []\n",
    "    cat_bottom_errors = []\n",
    "    for j in range(n_samples):\n",
    "        cont_error_map = error_maps_over_batch_size[i][j][:, cont_indices]\n",
    "        cat_error_map = error_maps_over_batch_size[i][j][:, cat_indices]\n",
    "        cont_entropy_map = entropy_maps_over_batch_size[i][j][:, cont_indices]\n",
    "        cat_entropy_map = entropy_maps_over_batch_size[i][j][:, cat_indices]\n",
    "        \n",
    "        all_cont_len = np.prod(cont_error_map.shape)\n",
    "        all_cat_len = np.prod(cont_error_map.shape)\n",
    "        cont_percentile = np.ceil(0.25*all_cont_len).astype(int)\n",
    "        cat_percentile = np.ceil(0.25*all_cat_len).astype(int)\n",
    "        \n",
    "        top_percentile_cat, bottom_percentile_cat = np.argsort(cat_entropy_map.flatten())[:cat_percentile], np.argsort(cat_entropy_map.flatten())[len(cat_entropy_map.flatten())-cat_percentile:]\n",
    "        top_percentile_cont, bottom_percentile_cont = np.argsort(cont_entropy_map.flatten())[:cont_percentile], np.argsort(cont_entropy_map.flatten())[len(cont_entropy_map.flatten())-cont_percentile:]\n",
    "        \n",
    "        mean_top_error_cat, mean_bottom_error_cat = np.mean(cat_error_map.flatten()[top_percentile_cat]), np.mean(cat_error_map.flatten()[bottom_percentile_cat])\n",
    "        mean_top_error_cont, mean_bottom_error_cont = np.mean(cont_error_map.flatten()[top_percentile_cont]), np.mean(cont_error_map.flatten()[bottom_percentile_cont])\n",
    "        \n",
    "        cont_top_errors.append(mean_top_error_cont)\n",
    "        cont_bottom_errors.append(mean_bottom_error_cont)\n",
    "        cat_top_errors.append(mean_top_error_cat)\n",
    "        cat_bottom_errors.append(mean_bottom_error_cat)\n",
    "    \n",
    "    display_data[i] = batch_size, (np.around(100-100*np.nanmean(cat_top_errors), 1), np.around(100*np.nanstd(cat_top_errors), 1)), (np.around(100-100*np.nanmean(cat_bottom_errors), 1), np.around(100*np.nanstd(cat_bottom_errors), 1)), (np.around(100-100*np.nanmean(cont_top_errors), 1), np.around(100*np.nanstd(cont_top_errors), 1)), (np.around(100-100*np.nanmean(cont_bottom_errors), 1), np.around(100*np.nanstd(cont_bottom_errors), 1))\n",
    "display_data_df = pd.DataFrame(data=display_data, columns=['Batch Size', f'Categorical Top {int(100*percentile)}%', f'Categorical Bottom {int(100*percentile)}%', f'Continuous Top {int(100*percentile)}%', f'Continuous Bottom {int(100*percentile)}%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ff8a9-2400-4041-8411-3d296ca4824e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aebf76-66d3-4264-a67e-0caadf4bd60a",
   "metadata": {},
   "source": [
    "## Results on the correlation of entropy and correctness for each batch size, as tables 25-28 in the Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba405ad-3c2c-4ce6-91ea-8070a757479a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_indices = dataset.train_cat_indices\n",
    "cont_indices = dataset.train_cont_indices\n",
    "display_data = np.zeros((len(batch_sizes), 7), dtype='object')\n",
    "\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "    cont_errors = []\n",
    "    cont_entropies = []\n",
    "    cat_errors = []\n",
    "    cat_entropies = []\n",
    "    for j in range(n_samples):\n",
    "        cont_error_map = error_maps_over_batch_size[i][j][:, cont_indices]\n",
    "        cat_error_map = error_maps_over_batch_size[i][j][:, cat_indices]\n",
    "        cont_entropy_map = entropy_maps_over_batch_size[i][j][:, cont_indices]\n",
    "        cat_entropy_map = entropy_maps_over_batch_size[i][j][:, cat_indices]\n",
    "        \n",
    "        cont_errors.append(np.nanmean(cont_error_map))\n",
    "        cont_entropies.append(np.nanmean(np.ma.masked_invalid(cont_entropy_map)))\n",
    "        cat_errors.append(np.nanmean(cat_error_map))\n",
    "        cat_entropies.append(np.nanmean(cat_entropy_map))\n",
    "    \n",
    "    cont_tau = kendalltau(1-np.array(cont_errors), cont_entropies)[0]\n",
    "    cat_tau = kendalltau(1-np.array(cat_errors), cat_entropies)[0]\n",
    "\n",
    "    display_data[i, 0] = batch_size\n",
    "    display_data[i, 1:4] = (np.around(100-100*np.nanmean(cat_errors), 1), np.around(100*np.nanstd(cat_errors), 1)), (np.around(np.nanmean(cat_entropies), 2), np.around(np.nanstd(cat_entropies), 2)), np.around(cat_tau, 2)\n",
    "    display_data[i, 4:] = (np.around(100-100*np.nanmean(cont_errors), 1), np.around(100*np.nanstd(cont_errors), 1)), (np.around(np.nanmean(cont_entropies), 2), np.around(np.nanstd(cont_entropies), 2)), np.around(cont_tau, 2)\n",
    "display_data_df = pd.DataFrame(data=display_data, columns=['Batch Size', 'Discrete Accuracy %', 'Discrete Entropy', 'Discrete Kendall\\'s Tau', 'Continuous Accuracy %', 'Continuous Entropy', 'Continuous Kendall\\'s Tau'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e3878-c079-4da9-9b9f-dc9c4a331faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60d8f0-b09c-434e-9da9-3e0c9e9d18f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
